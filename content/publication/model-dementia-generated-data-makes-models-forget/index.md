---
abstract: "Stable Diffusion revolutionised image creation from descriptive text.
  GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a
  variety of language tasks. ChatGPT introduced such language models to the
  general public. It is now clear that large language models (LLMs) are here to
  stay, and will bring about drastic change in the whole ecosystem of online
  text and images. In this paper we consider what the future might hold. What
  will happen to GPT-{n} once LLMs contribute much of the language found online?
  We find that use of model-generated content in training causes irreversible
  defects in the resulting models, where tails of the original content
  distribution disappear. We call this effect model dementia and show that it
  can occur in Variational Autoencoders (VAEs), Gaussian Mixture Models (GMMs)
  and LLMs. We build theoretical intuition behind the phenomenon and portray its
  ubiquity amongst all learned generative models. We demonstrate that it has to
  be taken seriously if we are to sustain the benefits of training from
  large-scale data scraped from the web. Indeed, the value of data collected
  about genuine human interactions with systems will be increasingly valuable in
  the presence of content generated by LLMs in data crawled from the Internet. "
draft: false
url_pdf: https://arxiv.org/abs/2305.17493
publication_types:
  - "3"
authors:
  - Ilia Shumailov
  - admin
  - Yiren Zhao
  - Yarin Gal
  - Nicolas Papernot
  - Ross Anderson
author_notes:
  - Equal contribution
  - Equal contribution
publication: ""
featured: false
date: 2023-05-30T10:12:10.475Z
url_slides: ""
title: "Model Dementia: Generated Data Makes Models Forget"
doi: https://doi.org/10.48550/arXiv.2305.17493
#tags:
#  - k-inflation
#  - Inflation
#  - Curved cosmology
##  - Power spectra
#  - Quantum initial conditions
#  - Change of variables
#  - Renormalised stress-energy tensor
#projects:
#  - cosmology
image:
  filename: null
  focal_point: Smart
  preview_only: false
---
