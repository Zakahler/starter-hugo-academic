---
doi: https://doi.org/10.1038/s41586-024-07566-y
abstract: Stable diffusion revolutionized image creation from descriptive text.
  GPT-2, GPT-3(.5) and GPT-4 demonstrated high performance across a variety of
  language tasks. ChatGPT introduced such language models to the public. It is
  now clear that generative artificial intelligence (AI) such as large language
  models (LLMs) is here to stay and will substantially change the ecosystem of
  online text and images. Here we consider what may happen to GPT-{n} once LLMs
  contribute much of the text found online. We find that indiscriminate use of
  model-generated content in training causes irreversible defects in the
  resulting models, in which tails of the original content distribution
  disappear. We refer to this effect as ‘model collapse’ and show that it can
  occur in LLMs as well as in variational autoencoders (VAEs) and Gaussian
  mixture models (GMMs). We build theoretical intuition behind the phenomenon
  and portray its ubiquity among all learned generative models. We demonstrate
  that it must be taken seriously if we are to sustain the benefits of training
  from large-scale data scraped from the web. Indeed, the value of data
  collected about genuine human interactions with systems will be increasingly
  valuable in the presence of LLM-generated content in data crawled from the
  Internet
draft: false
url_pdf: https://www.nature.com/articles/s41586-024-07566-y
publication_types:
  - "2"
authors:
  - admin
  - Ilia Shumailov
  - Yiren Zhao
  - Yarin Gal
  - Nicolas Papernot
  - Ross Anderson
author_notes:
  - Equal contribution
  - Equal contribution
publication: ""
featured: false
date: 2023-05-30T10:12:10.475Z
url_slides: ""
title: AI models collapse when trained on recursively generated data
image:
  filename: null
  focal_point: Smart
  preview_only: false
---
