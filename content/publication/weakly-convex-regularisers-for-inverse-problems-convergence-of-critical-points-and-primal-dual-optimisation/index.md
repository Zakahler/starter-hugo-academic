---
abstract: "Variational regularisation is the primary method for solving inverse
  problems, and recently there has been considerable work leveraging deeply
  learned regularisation for enhanced performance. However, few results exist
  addressing the convergence of such regularisation, particularly within the
  context of critical points as opposed to global minima. In this paper, we
  present a generalised formulation of convergent regularisation in terms of
  critical points, and show that this is achieved by a class of weakly convex
  regularisers. We prove convergence of the primal-dual hybrid gradient method
  for the associated variational problem, and, given a Kurdyka-Lojasiewicz
  condition, an O(log k/k) ergodic convergence rate. Finally, applying this
  theory to learned regularisation, we prove universal approximation for input
  weakly convex neural networks (IWCNN), and show empirically that IWCNNs can
  lead to improved performance of learned adversarial regularisers for computed
  tomography (CT) reconstruction. "
draft: false
url_pdf: https://arxiv.org/pdf/2402.01052.pdf
publication_types:
  - "3"
authors:
  - admin
  - Jeremy Budd
  - Subhadip Mukherjee
  - Carola Sch√∂nlieb
publication: ""
featured: false
date: 2024-02-20T16:58:47.220Z
url_slides: ""
title: "Weakly Convex Regularisers for Inverse Problems: Convergence of Critical
  Points and Primal-Dual Optimisation"
image:
  filename: ""
  focal_point: Smart
  preview_only: false
url_code: https://github.com/zakobian
doi: https://doi.org/10.48550/arXiv.2402.01052
---
